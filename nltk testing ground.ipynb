{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "electrical-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_verb(text):\n",
    "    pos_tagged = nltk.pos_tag(text)\n",
    "    print(pos_tagged)\n",
    "    verbs = [pos_tagged[0][0]] + [i[0] for i in pos_tagged[1:] if 'VB' in i[1]] # always include 1st word as nltk doesnt perform well here\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "least-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hit', 'NN'), ('and', 'CC'), ('simmer', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hit']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_non_verb(nltk.word_tokenize(\"hit and simmer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "republican-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('simmer.n.01'), Synset('simmer.v.01')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('simmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "random-character",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring to a boil', ' cover and simmer gently for 8 to 10 minutes', ' until soft and pulpy']\n",
      "bring to a boil\n",
      " cover and simmer gently for 8 to 10 minutes\n",
      " until soft and pulpy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2 = \"['mix water and flour, divide evenly between two: avocado, avocado, avocado's constant. preheat oven to 2000f', 'preheat the oven to 350f']\"\n",
    "text = \"['bring to a boil, cover and simmer gently for 8 to 10 minutes, until soft and pulpy']\"\n",
    "x = re.findall(\"[\\w|\\s]+\", text)\n",
    "print(x)\n",
    "[print(i) for i in x if any([j.isalnum() for j in i.split()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "invisible-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list(obj):\n",
    "    \"\"\"Convert dataframe object(string) to processable list\"\"\"\n",
    "    return [i for i in re.findall(\"[\\w\\s]+\", obj) if any([j.isalnum() for j in i.split()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "damaged-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring to a boil', ' cover and simmer gently for 8 to 10 minutes', ' until soft and pulpy']\n",
      "[['bring', 'to', 'a', 'boil'], ['cover', 'and', 'simmer', 'gently', 'for', '8', 'to', '10', 'minutes'], ['until', 'soft', 'and', 'pulpy']]\n"
     ]
    }
   ],
   "source": [
    "print(convert_list(text))\n",
    "steps = [nltk.word_tokenize(i) for i in convert_list(text)]\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "august-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bring', 'NN'), ('to', 'TO'), ('a', 'DT'), ('boil', 'NN')]\n",
      "[('bring', 'NN'), ('to', 'TO'), ('a', 'DT'), ('boil', 'NN')]\n",
      "[('cover', 'NN'), ('and', 'CC'), ('simmer', 'NN'), ('gently', 'RB'), ('for', 'IN'), ('8', 'CD'), ('to', 'TO'), ('10', 'CD'), ('minutes', 'NNS')]\n",
      "[('cover', 'NN'), ('and', 'CC'), ('simmer', 'NN'), ('gently', 'RB'), ('for', 'IN'), ('8', 'CD'), ('to', 'TO'), ('10', 'CD'), ('minutes', 'NNS')]\n",
      "[('until', 'IN'), ('soft', 'JJ'), ('and', 'CC'), ('pulpy', 'NN')]\n",
      "[('until', 'IN'), ('soft', 'JJ'), ('and', 'CC'), ('pulpy', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['bring'], ['cover'], ['until']]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[remove_non_verb(step) for step in steps if remove_non_verb(step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "charitable-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "rapid-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(word):\n",
    "    return 'v' in set(s.pos() for s in wn.synsets(word) if s.name().split('.')[0]==word)\n",
    "\n",
    "def remove_non_verb(text):\n",
    "    pos_tagged = nltk.pos_tag(text)\n",
    "    if pos_tagged:\n",
    "        verbs = [pos_tagged[0][0]] + [i[0] for i in pos_tagged[1:] if 'VB' in i[1]] # always include 1st word as nltk doesnt perform well here\n",
    "        return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "blessed-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'mix potatoes \", ' bacon ', ' green pepper ', ' onion ', \" eggs in a large bowl'\", \" 'add dressing and lemon juice'\", \" 'mix lightly'\", \" 'season to taste with salt and pepper'\", \" 'regrigerate'\"]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/test_small.csv', header=0).iloc[8:9]\n",
    "\n",
    "# convert dtype to list of string\n",
    "# split text sections by comma\n",
    "data['processed_steps'] = data['steps'].apply(lambda x: x[1:-1].split(\",\"))\n",
    "print(list(data['processed_steps'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "trained-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'mix\", 'potatoes'], ['bacon'], ['green', 'pepper'], ['onion'], ['eggs', 'in', 'a', 'large', 'bowl', \"'\"], [\"'add\", 'dressing', 'and', 'lemon', 'juice', \"'\"], [\"'mix\", 'lightly', \"'\"], [\"'season\", 'to', 'taste', 'with', 'salt', 'and', 'pepper', \"'\"], [\"'regrigerate\", \"'\"]]\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "data['processed_steps'] = data['processed_steps'].apply(lambda steps: [nltk.word_tokenize(step) for step in steps])\n",
    "print(list(data['processed_steps'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sophisticated-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'mix\"], ['bacon'], ['green'], ['onion'], ['eggs'], [\"'add\"], [\"'mix\"], [\"'season\", 'taste'], [\"'regrigerate\"]]\n"
     ]
    }
   ],
   "source": [
    "# drop non-verb words by pos-tag\n",
    "data['processed_steps'] = data['processed_steps'].apply(lambda steps: [remove_non_verb(step) for step in steps if remove_non_verb(step)])\n",
    "print(list(data['processed_steps'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "circular-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    [mix, green, egg, add, mix, season, taste]\n",
      "Name: processed_steps, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "# + flatten\n",
    "data['processed_steps'] = data['processed_steps'].apply(lambda steps: [word.translate(str.maketrans('', '', string.punctuation)) for step in steps for word in step])\n",
    "\n",
    "# lemmatization\n",
    "data['processed_steps'] = data['processed_steps'].apply(lambda steps: [lemmatizer.lemmatize(word, pos='v') for word in steps])\n",
    "\n",
    "# stopword removal\n",
    "# + drop by synsets\n",
    "data['processed_steps'] = data['processed_steps'].apply(lambda steps: [word for word in steps if (is_verb(word) and word not in stop_words)])\n",
    "\n",
    "print(data['processed_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
